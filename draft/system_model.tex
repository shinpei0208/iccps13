\section{System Model}
\label{sec:system_model}

This paper assumes that the system is composed of compute devices, input
sensors, and output actuators, in addition to typical workstation
components of the host computer.
In particular, we restrict our attention to the GPU as a compute device,
which best embraces a concept of many-core computing in the current
state of the art.
There are at least three \textit{device drivers} employed by the host
computer to manage the GPU, the sensors, and the actuators,
respectively.
We also assume that these devices are connect to the Peripheral
Component Interconnect Express (PCIe) bus to communicate with each
other.
The contribution of this paper, however, is not limited to the PCIe bus,
but is also applicable to any interconnect as far as it is mappable to
I/O address space.
There are two kinds of memory associated with the address space.
One is the host memory, also often referred to the main memory, which is
incorporated by the host computer.
The other is the device memory, which is encapsulated in the GPU.
Both of the memory types must be mappable to the PCIe bus.

The control algorithm is parallelized and offloaded to the GPU.
We specifically use CUDA to implement the algorithm, but any programming
language for the GPU is available under our model, because the
application programming interface (API) considered in this paper does
not depend on a specific programming language.
All input data come from the sensor modules, while all output data go to
the actuator modules.
The buffers for these data can be allocated to any place visible to I/O
address space.
According to the control system design, these data may or may not be
further copied to other buffers.
In either case, they are expressed as data \textit{arrays} in the
control algorithm.

There are several other assumptions that simplify our system model.
The system contains only the single real-time process (task) that
executes the control algorithm, except for trivial background jobs to
run the system.
Therefore, we ignore the problem of shared resources among multiple
tasks.
We also focus on a single instance of the GPU to implement the control
algorithm; this is not a conceptual limitation of this paper, and the
algorithm implementation could use multiple GPUs, as far as I/O address
space is unified among them.