\section{System Model}
\label{sec:system_model}

This paper assumes that the system is composed of compute devices, input
sensors, and output actuators, in addition to typical components of a
host computer.
We restrict our attention to GPUs as auxiliary compute devices,
which best embrace the concept of many-core computing in the state of
the art.
There must be at least three \textit{device drivers} employed by the
host computer to manage the GPU, sensors, and actuators,
respectively.
We assume that they are connected to the Peripheral Component
Interconnect Express (PCIe) bus of the host computer.
The contribution of this paper is not limited to the PCIe bus; it is
applicable to any interconnect that is mappable to I/O address space.
There are two kinds of memory associated with the address space.
One is the host memory, also referred to \emph{main memory}.
The other is the device memory, which is encapsulated in the GPU.
Both memory types are (and must be) mappable to the PCIe bus.

\textbf{Algorithm Implementation:}
The control algorithm is parallelized and offloaded to the GPU.
We use CUDA to implement the algorithm, but any programming
language for the GPU is available under our model, because the
application programming interface (API) considered in this paper does
not depend on a specific programming language.
All input data come from the sensor modules, while all output data go to
the actuator modules.
The buffers for the data can be allocated to any place visible to I/O
address space.
According to the control system design, the data may or may not be
further copied to other buffers.
In either case, they are expressed as data \textit{arrays} in the
control algorithm.

\textbf{PCIe BAR:}
The current form of the GPU is a PCI device.
Such a PCI-based compute device is typically designed to expose base
address registers (BARs) to the system, through which the CPU can access
specific areas of the device memory.
There are several BARs depending on the target device.
For example, NVIDIA GPUs provide the following BARs:
\begin{description}
 \item[BAR0] Memory-mapped I/O (MMIO) registers.
 \item[BAR1] Device memory aperture (windows).
 \item[BAR2] I/O port or complementary space of BAR1.
 \item[BAR3] Same as BAR2.
 \item[BAR5] I/O port.
 \item[BAR6] PCI ROM.
\end{description}
Often the BAR0 is used to access the control registers of the GPU while
the BAR1 makes the device memory visible to the CPU given their
different sizes of memory space.
Specifically the BAR1 region can be directly accessed by the GPU using
the unified memory addressing (UMA) mode that allows all memory objects
allocated to the host and the device memory to be referenced by the same
address space.
This BAR1 region can also be accessed by the CPU using the I/O remapping
function supported by the operating system kernel as well as I/O devices
by obtaining the physical I/O address of the corresponding BAR1 region.
This paper makes use of the latter technique to achieve direct data
communications between the GPU and I/O devices.

\textbf{Limitation:}
There are several other assumptions that simplify our system model.
The system contains only the single real-time process (task) that
executes the control algorithm, except for trivial background jobs to
run the system.
We ignore the problem of shared resources among multiple tasks, which
have been addressed elsewhere~\cite{Elliott_RTS12, Kato_RTAS11}.
We also focus on a single instance of the GPU to implement the control
algorithm.
This is not a conceptual limitation of this paper; the algorithm
implementation could just as easily use multiple GPUs.
